{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### head -1 data_combined_by_year/2016-train.csv > output.csv\n",
    "#### tail -n +2  data_combined_by_year/*.csv >> output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',index_col=False)\n",
    "dev = pd.read_csv('dev.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity between corresponding TF-IDF vectors from tf-idf value per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train['similarity_score']\n",
    "train_data = train.drop('similarity_score',axis=1)\n",
    "test_labels = dev['similarity_score']\n",
    "test_data = dev.drop('similarity_score',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        norm_text = norm_text.replace(char, ' ' + char + ' ')\n",
    "    return norm_text\n",
    "\n",
    "def tokenize(inputText, remove_stopwords=True,tagged=False, lemmatize=False):\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\",\" \", inputText)\n",
    "    text = review_text.lower().split()\n",
    "    words = []\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in text if not w in stops]\n",
    "    else:\n",
    "        return text\n",
    "    if not tagged: \n",
    "        return(words)\n",
    "    else:\n",
    "        lem = WordNetLemmatizer()\n",
    "        lemmatized_words = []\n",
    "        tagged_words=[]\n",
    "        filtered_tagged_words = []\n",
    "        tagged_text = nltk.pos_tag(words)\n",
    "        for word, tag in tagged_text:\n",
    "            tagged_words.append({\"word\": word, \"pos\": tag})\n",
    "        filtered_tagged_words = [word for word in tagged_words if word[\"pos\"] in [\"NN\", \"NNS\",\"NNP\"]]\n",
    "        if lemmatize:\n",
    "            for word in filtered_tagged_words:\n",
    "                lemmatized_words.append(lem.lemmatize(word[\"word\"]))\n",
    "            return(lemmatized_words)\n",
    "        else:\n",
    "            temp=[]\n",
    "            for entry in filtered_tagged_words:\n",
    "                temp.append(entry[\"word\"])\n",
    "            return temp\n",
    "        \n",
    "def text_to_sentence(inputText,tokenizer,remove_stopwords=True,tagged=False,lemmatize=False):\n",
    "    raw_sentences = tokenizer.tokenize(str(inputText).strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            temp2 = tokenize(raw_sentence,remove_stopwords,tagged,lemmatize)\n",
    "            if len(temp2) > 0:\n",
    "                sentences.append(temp2)\n",
    "    return sentences\n",
    "\n",
    "def avg_word_vectors(wordlist,size):\n",
    "    \"\"\"\n",
    "    returns a vector of zero for reviews containing words where none of them\n",
    "    met the min_count or were not seen in the training set\n",
    "    Otherwise return an average of the embeddings vectors\n",
    "    \"\"\"\n",
    "    sumvec=np.zeros(shape=(1,size))\n",
    "    wordcnt=0\n",
    "    for w in wordlist:\n",
    "        if w in model:\n",
    "            sumvec += model[w]\n",
    "            wordcnt +=1\n",
    "    if wordcnt ==0:\n",
    "        return sumvec\n",
    "    else:\n",
    "        return sumvec / wordcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', 'name', 'tom'], ['year', 'book', 'food']]\n"
     ]
    }
   ],
   "source": [
    "sample = \"Hello, My name is Tom. I am 20 years old with a love for books and food!!!\"\n",
    "sample= normalize_text(sample)\n",
    "print(text_to_sentence(sample,tokenizer,True,True,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data['cleaned_S1'] = train_data['sentence1'].apply(lambda row: [val for sublist in text_to_sentence(row,tokenizer) for val in sublist])\n",
    "train_data['cleaned_S2'] = train_data['sentence2'].apply(lambda row: [val for sublist in text_to_sentence(row,tokenizer) for val in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = train_data['cleaned_S1']+train_data['cleaned_S2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "dimsize=200\n",
    "model = Word2Vec(corpus, size=dimsize, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_S1=np.concatenate([avg_word_vectors(w,dimsize) for w in train_data['cleaned_S1']])\n",
    "train_S2=np.concatenate([avg_word_vectors(w,dimsize) for w in train_data['cleaned_S2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "sims = []\n",
    "for vec1,vec2 in zip(train_S1,train_S2):\n",
    "    sim = 1 - spatial.distance.cosine(vec1,vec2)\n",
    "    sims.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "with open('word2vec-output.txt','w') as fout: \n",
    "    for s in sims:\n",
    "        temp3 = (5-0)/(max(sims)-min(sims))*(s-max(sims))+5\n",
    "        #check why nan\n",
    "        if math.isnan(temp3) : temp3=0\n",
    "        print(temp3, file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(scaled_sims[i],train_labels[i], train_data.iloc[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCA_model(samples):\n",
    "    \"\"\"\n",
    "    Alternative to word2Vec for data vectorization\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    svd = TruncatedSVD(n_components=5, random_state=42)\n",
    "    pca = make_pipeline(vectorizer, svd, Normalizer(copy=False))\n",
    "    model = pca.fit(samples)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
