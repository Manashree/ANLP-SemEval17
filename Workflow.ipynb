{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### head -1 data_combined_by_year/2016-train.csv > output.csv\n",
    "#### tail -n +2  data_combined_by_year/*.csv >> output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy import spatial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',index_col=False)\n",
    "dev = pd.read_csv('dev.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity between word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train['similarity_score']\n",
    "train_data = train.drop('similarity_score',axis=1)\n",
    "test_labels = dev['similarity_score']\n",
    "test_data = dev.drop('similarity_score',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data['cleaned_S1'] = train_data['sentence1'].apply(lambda row: [val for sublist in text_to_sentence(row,tokenizer,True,False,True) for val in sublist])\n",
    "train_data['cleaned_S2'] = train_data['sentence2'].apply(lambda row: [val for sublist in text_to_sentence(row,tokenizer,True,False,True) for val in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        norm_text = norm_text.replace(char, ' ' + char + ' ')\n",
    "    return norm_text\n",
    "\n",
    "def tokenize(inputText, remove_stopwords=True,tagged=False, lemmatize=False):\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\",\" \", inputText)\n",
    "    text = review_text.lower().split()\n",
    "    words = []\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in text if not w in stops]\n",
    "    else:\n",
    "        return text\n",
    "    if not tagged: \n",
    "        return(words)\n",
    "    else:\n",
    "        lem = WordNetLemmatizer()\n",
    "        lemmatized_words = []\n",
    "        tagged_words=[]\n",
    "        filtered_tagged_words = []\n",
    "        tagged_text = nltk.pos_tag(words)\n",
    "        for word, tag in tagged_text:\n",
    "            tagged_words.append({\"word\": word, \"pos\": tag})\n",
    "        filtered_tagged_words = [word for word in tagged_words if word[\"pos\"] in [\"NN\", \"NNS\",\"NNP\",\"NNPS\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]]\n",
    "        if lemmatize:\n",
    "            for word in filtered_tagged_words:\n",
    "                lemmatized_words.append(lem.lemmatize(word[\"word\"]))\n",
    "            return(lemmatized_words)\n",
    "        else:\n",
    "            temp=[]\n",
    "            for entry in filtered_tagged_words:\n",
    "                temp.append(entry[\"word\"])\n",
    "            return temp\n",
    "        \n",
    "def text_to_sentence(inputText,tokenizer,remove_stopwords=True,tagged=False,lemmatize=False):\n",
    "    raw_sentences = tokenizer.tokenize(str(inputText).strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            temp2 = tokenize(raw_sentence,remove_stopwords,tagged,lemmatize)\n",
    "            if len(temp2) > 0:\n",
    "                sentences.append(temp2)\n",
    "    return sentences\n",
    "\n",
    "#Build word vector for training set by using the average value of all word vectors in the sentence, then scale\n",
    "def buildWordVector(text, size, model):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            temp = model[word].reshape(1, size)\n",
    "            vec += temp\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = \"Hello, My name is Tom. I am 20 years old with a love for books and food!!!\"\n",
    "sample= normalize_text(sample)\n",
    "print(text_to_sentence(sample,tokenizer,True,True,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "def PCA_model(samples):\n",
    "    \"\"\"\n",
    "    Alternative to word2Vec for data vectorization\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    svd = TruncatedSVD(n_components=5, random_state=42)\n",
    "    pca = make_pipeline(vectorizer, svd, Normalizer(copy=False))\n",
    "    model = pca.fit(samples)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns 2 arrays with vectors for sentences\n",
    "#requirements: GoogleNews-vectors-negative300.bin in same folder if using that model\n",
    "\n",
    "dimsize=300\n",
    "def createWord2vecFeature(sentence1_arr,sentence2_arr,dimsize=300,default_model=\"other\"):\n",
    "    #sentence_arr is the column containing cleaned sentences (sentence1[] & sentence2[])\n",
    "    if default_model == \"google\":\n",
    "        model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    else:\n",
    "        corpus = sentence1_arr + sentence2_arr\n",
    "        model = Word2Vec(corpus, size=dimsize, window=5, min_count=5, workers=4)\n",
    "    train_S1=np.concatenate([buildWordVector(w,dimsize,model) for w in sentence1_arr])\n",
    "    train_S2=np.concatenate([buildWordVector(w,dimsize,model) for w in sentence2_arr])\n",
    "    return calculateSimilarity(train_S1,train_S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns similarity array for each entry in arr1 and arr2\n",
    "from scipy import spatial\n",
    "def calculateSimilarity(arr1,arr2):\n",
    "    sims = []\n",
    "    for vec1,vec2 in zip(arr1,arr2):\n",
    "        sim = 1 - spatial.distance.cosine(vec1,vec2)\n",
    "        sims.append(sim)\n",
    "    return scaleSimilarity(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def scaleSimilarity(arr, scale_lower_bound=0, scale_upper_bound=5,write_to_file=False,file_name=\"temp.csv\"):\n",
    "    temp_arr=[]\n",
    "    temp_val=0\n",
    "    for s in arr:\n",
    "        max_sim = max(arr)\n",
    "        min_sim = min(arr)\n",
    "        temp_val = (scale_upper_bound-scale_lower_bound)/(max_sim-min_sim)*(s-max_sim)+scale_upper_bound\n",
    "        #check why nan\n",
    "        if math.isnan(temp_val) : temp_val=0\n",
    "        temp_arr.append(temp_val)\n",
    "    if write_to_file:\n",
    "        with open(file_name,'w') as fout: \n",
    "            for item in temp_arr:\n",
    "                print(item,fout=file_name)\n",
    "    else:\n",
    "        return temp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TFIDFModel(sentence1_arr,sentence2_arr):\n",
    "    temp_s1 =[]\n",
    "    temp_s2 =[]\n",
    "    items=[]\n",
    "    \n",
    "    corpus = sentence1_arr + sentence2_arr\n",
    "    \n",
    "    for item in corpus:\n",
    "        items.append(\" \".join(item))\n",
    "\n",
    "    for item in sentence1_arr:\n",
    "        temp_s1.append(\" \".join(item))\n",
    "\n",
    "    for item in sentence2_arr:\n",
    "        temp_s2.append(\" \".join(item))\n",
    "\n",
    "    model = PCA_model(items)\n",
    "    vec1 = model.transform(temp_s1)\n",
    "    vec2 = model.transform(temp_s2)\n",
    "    return calculateSimilarity(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = createWord2vecFeature(train_data['cleaned_S1'],train_data['cleaned_S2'])\n",
    "s2 = TFIDFModel(train_data['cleaned_S1'],train_data['cleaned_S2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6 4.99858650928 4.99528756196\n",
      "2.0 4.88620399506 4.79515297126\n",
      "2.2 4.95829897141 4.72730047087\n",
      "1.6 4.90017293757 4.66259643043\n",
      "4.2 4.99640191964 4.95254318146\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(train_labels[i],s1[i],s2[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
