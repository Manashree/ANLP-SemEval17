{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### head -1 data_combined_by_year/2016-train.csv > output.csv\n",
    "#### tail -n +2  data_combined_by_year/*.csv >> output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy import spatial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14028, 5)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('final.csv',index_col=False)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity between word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train['similarity_score']\n",
    "train_data = train.drop('similarity_score',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data['cleaned_S1'] = train_data['sentence1'].apply(lambda row: [val for sublist in text_to_sentence(row,tokenizer,True,False,True) for val in sublist])\n",
    "train_data['cleaned_S2'] = train_data['sentence2'].apply(lambda row: [val for sublist in text_to_sentence(row,tokenizer,True,False,True) for val in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['cleaned_tagged_S1'] = train_data['sentence1'].apply(lambda row: [val for sublist in text_to_sentence(row,tokenizer,True,True,True) for val in sublist])\n",
    "train_data['cleaned_tagged_S2'] = train_data['sentence2'].apply(lambda row: [val for sublist in text_to_sentence(row,tokenizer,True,True,True) for val in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec_feat = pd.read_csv('word2vec-feature-final.csv',index_col=False)\n",
    "train_data['word2vec_cosine']=word2vec_feat['word2vec_feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        norm_text = norm_text.replace(char, ' ' + char + ' ')\n",
    "    return norm_text\n",
    "\n",
    "def tokenize(inputText, remove_stopwords=True,tagged=False, lemmatize=False):\n",
    "    POS_TAGS=[\"NN\", \"NNS\",\"NNP\",\"NNPS\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\",\" \", inputText)\n",
    "    text = review_text.lower().split()\n",
    "    words = []\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in text if not w in stops]\n",
    "    else:\n",
    "        return text\n",
    "    if not tagged: \n",
    "        return(words)\n",
    "    else:\n",
    "        lem = WordNetLemmatizer()\n",
    "        lemmatized_words = []\n",
    "        tagged_words=[]\n",
    "        filtered_tagged_words = []\n",
    "        tagged_text = nltk.pos_tag(words)\n",
    "        for word, tag in tagged_text:\n",
    "            tagged_words.append({\"word\": word, \"pos\": tag})\n",
    "        filtered_tagged_words = [word for word in tagged_words if word[\"pos\"] in POS_TAGS]\n",
    "        if lemmatize:\n",
    "            for word in filtered_tagged_words:\n",
    "                lemmatized_words.append(lem.lemmatize(word[\"word\"]))\n",
    "            return(lemmatized_words)\n",
    "        else:\n",
    "            temp=[]\n",
    "            for entry in filtered_tagged_words:\n",
    "                temp.append(entry[\"word\"])\n",
    "            return temp\n",
    "        \n",
    "def text_to_sentence(inputText,tokenizer,remove_stopwords=True,tagged=False,lemmatize=False):\n",
    "    raw_sentences = tokenizer.tokenize(str(inputText).strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            temp2 = tokenize(raw_sentence,remove_stopwords,tagged,lemmatize)\n",
    "            if len(temp2) > 0:\n",
    "                sentences.append(temp2)\n",
    "    return sentences\n",
    "\n",
    "#Build word vector for training set by using the average value of all word vectors in the sentence, then scale\n",
    "def buildWordVector(text, size, model):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            temp = model[word].reshape(1, size)\n",
    "            vec += temp\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "def POS_tag(text):\n",
    "    tags =[]\n",
    "    tagged_text = nltk.pos_tag(text.split())\n",
    "    for word, tag in tagged_text:\n",
    "        tags.append(tag)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = \"Hello, My name is Tom. I am 20 years old with a love for books and food!!!\"\n",
    "sample= normalize_text(sample)\n",
    "print(text_to_sentence(sample,tokenizer,True,True,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "def PCA_model(samples):\n",
    "    #Alternative to word2Vec for data vectorization\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    svd = TruncatedSVD(n_components=5, random_state=42)\n",
    "    pca = make_pipeline(vectorizer, svd, Normalizer(copy=False))\n",
    "    model = pca.fit(samples)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns 2 arrays with vectors for sentences\n",
    "#requirements: GoogleNews-vectors-negative300.bin in same folder if using that model\n",
    "\n",
    "dimsize=300\n",
    "def createWord2vecFeature(sentence1_arr,sentence2_arr,dimsize=300,default_model=\"other\"):\n",
    "    #sentence_arr is the column containing cleaned sentences (sentence1[] & sentence2[])\n",
    "    if default_model == \"google\":\n",
    "        model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    else:\n",
    "        corpus = sentence1_arr + sentence2_arr\n",
    "        model = Word2Vec(corpus, size=dimsize, window=5, min_count=5, workers=4)\n",
    "    train_S1=np.concatenate([buildWordVector(w,dimsize,model) for w in sentence1_arr])\n",
    "    train_S2=np.concatenate([buildWordVector(w,dimsize,model) for w in sentence2_arr])\n",
    "    return calculateSimilarity(train_S1,train_S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns similarity array for each entry in arr1 and arr2\n",
    "# spatial.distance.cosine computes the distance, and not the similarity. So, you must subtract the value from 1 to get the similarity.\n",
    "from scipy import spatial\n",
    "def calculateSimilarity(arr1,arr2,scale_flag=True):\n",
    "    sims = []\n",
    "    for vec1,vec2 in zip(arr1,arr2):\n",
    "        sim = 1 - spatial.distance.cosine(vec1,vec2)\n",
    "        sims.append(sim)\n",
    "    if scale_flag:\n",
    "        return scaleSimilarity(sims)\n",
    "    else:\n",
    "        return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def scaleSimilarity(arr, scale_lower_bound=0, scale_upper_bound=5,write_to_file=False,file_name=\"temp.csv\"):\n",
    "    temp_arr=[]\n",
    "    temp_val=0\n",
    "    for s in arr:\n",
    "        max_sim = max(arr)\n",
    "        min_sim = min(arr)\n",
    "        temp_val = (scale_upper_bound-scale_lower_bound)/(max_sim-min_sim)*(s-max_sim)+scale_upper_bound\n",
    "        #check why nan\n",
    "        if math.isnan(temp_val) : temp_val=0\n",
    "        temp_arr.append(temp_val)\n",
    "    if write_to_file:\n",
    "        with open(file_name,'w') as fout: \n",
    "            for item in temp_arr:\n",
    "                print(item,fout=file_name)\n",
    "    else:\n",
    "        return temp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TFIDFModel(sentence1_arr,sentence2_arr,scale_flag=True):\n",
    "    temp_s1 =[]\n",
    "    temp_s2 =[]\n",
    "    items=[]\n",
    "    \n",
    "    corpus = sentence1_arr + sentence2_arr\n",
    "    \n",
    "    for item in corpus:\n",
    "        items.append(\" \".join(item))\n",
    "\n",
    "    for item in sentence1_arr:\n",
    "        temp_s1.append(\" \".join(item))\n",
    "\n",
    "    for item in sentence2_arr:\n",
    "        temp_s2.append(\" \".join(item))\n",
    "\n",
    "    model = PCA_model(items)\n",
    "    vec1 = model.transform(temp_s1)\n",
    "    vec2 = model.transform(temp_s2)\n",
    "    return calculateSimilarity(vec1, vec2,scale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#s1 = createWord2vecFeature(train_data['cleaned_S1'],train_data['cleaned_S2'])\n",
    "train_data['TFIDF_cosine']=TFIDFModel(train_data['cleaned_S1'],train_data['cleaned_S2'], scale_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_jaccard_index(set_1, set_2):\n",
    "    n = len(set_1.intersection(set_2))\n",
    "    return n / float(len(set_1) + len(set_2) - n)\n",
    "\n",
    "def jaccardFeature(arr1,arr2,feature_type):\n",
    "    missing_cnt=0;\n",
    "    for i in range(len(arr1)):\n",
    "        try:\n",
    "            train_data.ix[i,feature_type] = compute_jaccard_index(set(arr1[i]),set(arr2[i]))\n",
    "        except ZeroDivisionError:\n",
    "            missing_cnt+=1\n",
    "            train_data.ix[i,feature_type]=0\n",
    "    print('Missing values count for type:',feature_type,\" is \",missing_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LCS_length(list_1, list_2):\n",
    "    table = [[0] * (len(list_2) + 1) for _ in range(len(list_1) + 1)]\n",
    "    for i, ca in enumerate(list_1, 1):\n",
    "        for j, cb in enumerate(list_2, 1):\n",
    "            table[i][j] = (\n",
    "                table[i - 1][j - 1] + 1 if ca == cb else\n",
    "                max(table[i][j - 1], table[i - 1][j]))\n",
    "    return table[-1][-1]\n",
    "\n",
    "def LCS_feature(arr1, arr2):\n",
    "    for i in range(len(arr1)):\n",
    "        temp_avg = np.mean([len(arr1[i]),len(arr2[i])])\n",
    "        train_data.ix[i,'longest_common_subseq'] = LCS_length(arr1[i],arr2[i])/temp_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_dice_coefficient(set_1, set_2):\n",
    "    n = len(set_1.intersection(set_2))\n",
    "    return 2*n / float(len(set_1) + len(set_2))\n",
    "\n",
    "def diceFeature(arr1,arr2):\n",
    "    missing_cnt=0;\n",
    "    for i in range(len(arr1)):\n",
    "        try:\n",
    "            train_data.ix[i,'dice_coefficient'] = compute_dice_coefficient(set(arr1[i]),set(arr2[i]))\n",
    "        except ZeroDivisionError:\n",
    "            missing_cnt+=1\n",
    "            train_data.ix[i,'dice_coefficient']=0\n",
    "    print('Missing values count for dice_coefficient:',\" is \",missing_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count for type: word_jaccard  is  6\n",
      "Missing values count for type: POS_jaccard  is  0\n",
      "Missing values count for dice_coefficient:  is  6\n"
     ]
    }
   ],
   "source": [
    "LCS_feature(train_data['cleaned_S1'],train_data['cleaned_S2'])\n",
    "jaccardFeature(train_data['cleaned_S1'],train_data['cleaned_S2'],'word_jaccard')\n",
    "jaccardFeature(train_data['sentence1'].apply(POS_tag), train_data['sentence2'].apply(POS_tag),'POS_jaccard')\n",
    "diceFeature(train_data['cleaned_S1'],train_data['cleaned_S2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDF = train_data\n",
    "newDF['original_score'] = train_labels\n",
    "train_data.to_csv('final-modified.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
