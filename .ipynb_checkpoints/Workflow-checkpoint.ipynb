{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',index_col=False)\n",
    "dev = pd.read_csv('dev.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity between corresponding TF-IDF vectors from tf-idf value per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train['similarity_score']\n",
    "train_data = train.drop('similarity_score',axis=1)\n",
    "test_labels = dev['similarity_score']\n",
    "test_data = dev.drop('similarity_score',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        norm_text = norm_text.replace(char, ' ' + char + ' ')\n",
    "    return norm_text\n",
    "\n",
    "def tokenize(inputText, remove_stopwords=True,tagged=False, lemmatize=False):\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\",\" \", inputText)\n",
    "    text = review_text.lower().split()\n",
    "    words = []\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in text if not w in stops]\n",
    "    else:\n",
    "        return text\n",
    "    if not tagged: \n",
    "        return(words)\n",
    "    else:\n",
    "        lem = WordNetLemmatizer()\n",
    "        lemmatized_words = []\n",
    "        tagged_words=[]\n",
    "        filtered_tagged_words = []\n",
    "        tagged_text = nltk.pos_tag(words)\n",
    "        for word, tag in tagged_text:\n",
    "            tagged_words.append({\"word\": word, \"pos\": tag})\n",
    "        filtered_tagged_words = [word for word in tagged_words if word[\"pos\"] in [\"NN\", \"NNS\",\"NNP\"]]\n",
    "        if lemmatize:\n",
    "            for word in filtered_tagged_words:\n",
    "                lemmatized_words.append(lem.lemmatize(word[\"word\"]))\n",
    "            return(lemmatized_words)\n",
    "        else:\n",
    "            temp=[]\n",
    "            for entry in filtered_tagged_words:\n",
    "                temp.append(entry[\"word\"])\n",
    "            return temp\n",
    "        \n",
    "def text_to_sentence(inputText,tokenizer,remove_stopwords=True,tagged=False,lemmatize=False):\n",
    "    raw_sentences = tokenizer.tokenize(inputText.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            temp2 = tokenize(raw_sentence,remove_stopwords,tagged,lemmatize)\n",
    "            if len(temp2) > 0:\n",
    "                sentences.append(temp2)\n",
    "    return sentences\n",
    "\n",
    "def avg_word_vectors(wordlist,size):\n",
    "    \"\"\"\n",
    "    returns a vector of zero for reviews containing words where none of them\n",
    "    met the min_count or were not seen in the training set\n",
    "    Otherwise return an average of the embeddings vectors\n",
    "    \"\"\"\n",
    "    sumvec=np.zeros(shape=(1,size))\n",
    "    wordcnt=0\n",
    "    for w in wordlist:\n",
    "        if w in model:\n",
    "            sumvec += model[w]\n",
    "            wordcnt +=1\n",
    "    if wordcnt ==0:\n",
    "        return sumvec\n",
    "    else:\n",
    "        return sumvec / wordcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', 'name', 'tom'], ['year', 'book', 'food']]\n"
     ]
    }
   ],
   "source": [
    "sample = \"Hello, My name is Tom. I am 20 years old with a love for books and food!!!\"\n",
    "sample= normalize_text(sample)\n",
    "print(text_to_sentence(sample,tokenizer,True,True,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
