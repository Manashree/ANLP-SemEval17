{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### head -1 data_combined_by_year/2016-train.csv > output.csv\n",
    "#### tail -n +2  data_combined_by_year/*.csv >> output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manashree/miniconda2/envs/Mani/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy import spatial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11222, 5) (11222, 1)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv',index_col=False)\n",
    "dev = pd.read_csv('dev.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity between word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train['similarity_score']\n",
    "train_data = train.drop('similarity_score',axis=1)\n",
    "test_labels = dev['similarity_score']\n",
    "test_data = dev.drop('similarity_score',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data['cleaned_S1'] = train_data['sentence1'].apply(lambda row: [val for sublist in text_to_sentence(row,tokenizer,True,False,True) for val in sublist])\n",
    "train_data['cleaned_S2'] = train_data['sentence2'].apply(lambda row: [val for sublist in text_to_sentence(row,tokenizer,True,False,True) for val in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['cleaned_tagged_S1'] = train_data['sentence1'].apply(lambda row: [val for sublist in text_to_sentence(row,tokenizer,True,True,True) for val in sublist])\n",
    "train_data['cleaned_tagged_S2'] = train_data['sentence2'].apply(lambda row: [val for sublist in text_to_sentence(row,tokenizer,True,True,True) for val in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec_feat = pd.read_csv('word2vec-feature-train.csv',index_col=False)\n",
    "train_data['word2vec_cosine']=word2vec_feat['word2vec_feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        norm_text = norm_text.replace(char, ' ' + char + ' ')\n",
    "    return norm_text\n",
    "\n",
    "def tokenize(inputText, remove_stopwords=True,tagged=False, lemmatize=False):\n",
    "    POS_TAGS=[\"NN\", \"NNS\",\"NNP\",\"NNPS\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\",\" \", inputText)\n",
    "    text = review_text.lower().split()\n",
    "    words = []\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in text if not w in stops]\n",
    "    else:\n",
    "        return text\n",
    "    if not tagged: \n",
    "        return(words)\n",
    "    else:\n",
    "        lem = WordNetLemmatizer()\n",
    "        lemmatized_words = []\n",
    "        tagged_words=[]\n",
    "        filtered_tagged_words = []\n",
    "        tagged_text = nltk.pos_tag(words)\n",
    "        for word, tag in tagged_text:\n",
    "            tagged_words.append({\"word\": word, \"pos\": tag})\n",
    "        filtered_tagged_words = [word for word in tagged_words if word[\"pos\"] in POS_TAGS]\n",
    "        if lemmatize:\n",
    "            for word in filtered_tagged_words:\n",
    "                lemmatized_words.append(lem.lemmatize(word[\"word\"]))\n",
    "            return(lemmatized_words)\n",
    "        else:\n",
    "            temp=[]\n",
    "            for entry in filtered_tagged_words:\n",
    "                temp.append(entry[\"word\"])\n",
    "            return temp\n",
    "        \n",
    "def text_to_sentence(inputText,tokenizer,remove_stopwords=True,tagged=False,lemmatize=False):\n",
    "    raw_sentences = tokenizer.tokenize(str(inputText).strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            temp2 = tokenize(raw_sentence,remove_stopwords,tagged,lemmatize)\n",
    "            if len(temp2) > 0:\n",
    "                sentences.append(temp2)\n",
    "    return sentences\n",
    "\n",
    "#Build word vector for training set by using the average value of all word vectors in the sentence, then scale\n",
    "def buildWordVector(text, size, model):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            temp = model[word].reshape(1, size)\n",
    "            vec += temp\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "def POS_tag(text):\n",
    "    tags =[]\n",
    "    tagged_text = nltk.pos_tag(text.split())\n",
    "    for word, tag in tagged_text:\n",
    "        tags.append(tag)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = \"Hello, My name is Tom. I am 20 years old with a love for books and food!!!\"\n",
    "sample= normalize_text(sample)\n",
    "print(text_to_sentence(sample,tokenizer,True,True,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "def PCA_model(samples):\n",
    "    #Alternative to word2Vec for data vectorization\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    svd = TruncatedSVD(n_components=5, random_state=42)\n",
    "    pca = make_pipeline(vectorizer, svd, Normalizer(copy=False))\n",
    "    model = pca.fit(samples)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns 2 arrays with vectors for sentences\n",
    "#requirements: GoogleNews-vectors-negative300.bin in same folder if using that model\n",
    "\n",
    "dimsize=300\n",
    "def createWord2vecFeature(sentence1_arr,sentence2_arr,dimsize=300,default_model=\"other\"):\n",
    "    #sentence_arr is the column containing cleaned sentences (sentence1[] & sentence2[])\n",
    "    if default_model == \"google\":\n",
    "        model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    else:\n",
    "        corpus = sentence1_arr + sentence2_arr\n",
    "        model = Word2Vec(corpus, size=dimsize, window=5, min_count=5, workers=4)\n",
    "    train_S1=np.concatenate([buildWordVector(w,dimsize,model) for w in sentence1_arr])\n",
    "    train_S2=np.concatenate([buildWordVector(w,dimsize,model) for w in sentence2_arr])\n",
    "    return calculateSimilarity(train_S1,train_S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns similarity array for each entry in arr1 and arr2\n",
    "# spatial.distance.cosine computes the distance, and not the similarity. So, you must subtract the value from 1 to get the similarity.\n",
    "from scipy import spatial\n",
    "def calculateSimilarity(arr1,arr2,scale_flag=True):\n",
    "    sims = []\n",
    "    for vec1,vec2 in zip(arr1,arr2):\n",
    "        sim = 1 - spatial.distance.cosine(vec1,vec2)\n",
    "        sims.append(sim)\n",
    "    if scale_flag:\n",
    "        return scaleSimilarity(sims)\n",
    "    else:\n",
    "        return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def scaleSimilarity(arr, scale_lower_bound=0, scale_upper_bound=5,write_to_file=False,file_name=\"temp.csv\"):\n",
    "    temp_arr=[]\n",
    "    temp_val=0\n",
    "    for s in arr:\n",
    "        max_sim = max(arr)\n",
    "        min_sim = min(arr)\n",
    "        temp_val = (scale_upper_bound-scale_lower_bound)/(max_sim-min_sim)*(s-max_sim)+scale_upper_bound\n",
    "        #check why nan\n",
    "        if math.isnan(temp_val) : temp_val=0\n",
    "        temp_arr.append(temp_val)\n",
    "    if write_to_file:\n",
    "        with open(file_name,'w') as fout: \n",
    "            for item in temp_arr:\n",
    "                print(item,fout=file_name)\n",
    "    else:\n",
    "        return temp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TFIDFModel(sentence1_arr,sentence2_arr,scale_flag=True):\n",
    "    temp_s1 =[]\n",
    "    temp_s2 =[]\n",
    "    items=[]\n",
    "    \n",
    "    corpus = sentence1_arr + sentence2_arr\n",
    "    \n",
    "    for item in corpus:\n",
    "        items.append(\" \".join(item))\n",
    "\n",
    "    for item in sentence1_arr:\n",
    "        temp_s1.append(\" \".join(item))\n",
    "\n",
    "    for item in sentence2_arr:\n",
    "        temp_s2.append(\" \".join(item))\n",
    "\n",
    "    model = PCA_model(items)\n",
    "    vec1 = model.transform(temp_s1)\n",
    "    vec2 = model.transform(temp_s2)\n",
    "    return calculateSimilarity(vec1, vec2,scale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#s1 = createWord2vecFeature(train_data['cleaned_S1'],train_data['cleaned_S2'])\n",
    "train_data['TFIDF_cosine']=TFIDFModel(train_data['cleaned_S1'],train_data['cleaned_S2'], scale_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_jaccard_index(set_1, set_2):\n",
    "    n = len(set_1.intersection(set_2))\n",
    "    return n / float(len(set_1) + len(set_2) - n)\n",
    "\n",
    "def jaccardFeature(arr1,arr2,feature_type):\n",
    "    missing_cnt=0;\n",
    "    for i in range(len(arr1)):\n",
    "        try:\n",
    "            train_data.ix[i,feature_type] = compute_jaccard_index(set(arr1[i]),set(arr2[i]))\n",
    "        except ZeroDivisionError:\n",
    "            missing_cnt+=1\n",
    "            train_data.ix[i,feature_type]=0\n",
    "    print('Missing values count for type:',feature_type,\" is \",missing_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LCS_length(list_1, list_2):\n",
    "    table = [[0] * (len(list_2) + 1) for _ in range(len(list_1) + 1)]\n",
    "    for i, ca in enumerate(list_1, 1):\n",
    "        for j, cb in enumerate(list_2, 1):\n",
    "            table[i][j] = (\n",
    "                table[i - 1][j - 1] + 1 if ca == cb else\n",
    "                max(table[i][j - 1], table[i - 1][j]))\n",
    "    return table[-1][-1]\n",
    "\n",
    "def LCS_feature(arr1, arr2):\n",
    "    for i in range(len(arr1)):\n",
    "        temp_avg = np.mean([len(arr1[i]),len(arr2[i])])\n",
    "        train_data.ix[i,'longest_common_subseq'] = LCS_length(arr1[i],arr2[i])/temp_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_dice_coefficient(set_1, set_2):\n",
    "    n = len(set_1.intersection(set_2))\n",
    "    return 2*n / float(len(set_1) + len(set_2))\n",
    "\n",
    "def diceFeature(arr1,arr2):\n",
    "    missing_cnt=0;\n",
    "    for i in range(len(arr1)):\n",
    "        try:\n",
    "            train_data.ix[i,'dice_coefficient'] = compute_dice_coefficient(set(arr1[i]),set(arr2[i]))\n",
    "        except ZeroDivisionError:\n",
    "            missing_cnt+=1\n",
    "            train_data.ix[i,'dice_coefficient']=0\n",
    "    print('Missing values count for dice_coefficient:',\" is \",missing_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LCS_feature(train_data['cleaned_S1'],train_data['cleaned_S2'])\n",
    "jaccardFeature(train_data['cleaned_S1'],train_data['cleaned_S2'],'word_jaccard')\n",
    "jaccardFeature(train_data['sentence1'].apply(POS_tag), train_data['sentence2'].apply(POS_tag),'POS_jaccard')\n",
    "diceFeature(train_data['cleaned_S1'],train_data['cleaned_S2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDF = train_data\n",
    "newDF['original_score'] = train_labels\n",
    "train_data.to_csv('train-modified.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
